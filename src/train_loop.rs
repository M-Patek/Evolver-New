// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use crate::core::algebra::{Vector, Matrix, Float, MANIFOLD_DIM};
use crate::core::affine::AffineTuple;
use crate::core::neuron::HTPNeuron;
use crate::core::oracle::LogicOracle;
use crate::core::param::HyperParams;
use crate::topology::tensor::HyperTensor;

/// ğŸ‹ï¸ TrainingLoop: é€»è¾‘è¿›åŒ–è®­ç»ƒå™¨
///
/// White-Box æ¶æ„æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
/// 1. Gradient Descent (é€šè¯†å­¦ä¹ ): é€šè¿‡å¤§é‡æ ·æœ¬æ…¢æ…¢è°ƒæ•´æƒé‡ï¼Œå­¦ä¹ é€šç”¨é€»è¾‘æ¨¡å¼ã€‚
/// 2. Algebraic Solver (é¡¿æ‚Ÿ/One-Shot): é€šè¿‡ä»£æ•°é€†è¿ç®—ï¼Œç¬é—´å­¦ä¼šç‰¹å®šäº‹å®ã€‚
pub struct TrainingLoop {
    params: HyperParams,
    optimizer: SimpleOptimizer,
}

impl TrainingLoop {
    pub fn new(params: HyperParams) -> Self {
        TrainingLoop {
            params: params.clone(),
            optimizer: SimpleOptimizer::new(params.learning_rate),
        }
    }

    /// ğŸ“‰ Mode 1: Gradient Descent Step (åå‘ä¼ æ’­)
    /// é€‚ç”¨äºå­¦ä¹ é€šç”¨è§„å¾‹ (Generalization)
    pub fn train_step_sgd(
        &mut self, 
        inputs: &[AffineTuple], 
        target_root: &AffineTuple
    ) -> Float {
        // 1. Forward Pass (with Trace)
        // å¼€å¯ training_mode=true ä»¥è®°å½•æ¢¯åº¦ç£å¸¦
        let hyper_tensor = HyperTensor::forward(inputs, true);
        
        // 2. Compute Loss
        // L = || Prediction - Target ||^2
        // è¿™é‡Œç®€åŒ–ä¸ºåªè®¡ç®— Translation (Bias) çš„è¯¯å·®ï¼Œå®é™…åº”åŒ…å« Linear éƒ¨åˆ†
        let loss = LogicOracle::calculate_loss(
            &hyper_tensor.root.translation, 
            &target_root.translation
        );

        // 3. Backward Pass (Auto-Diff)
        // ä» Trace ä¸­åå‘æ¨å¯¼æ¢¯åº¦
        if let Some(trace) = &hyper_tensor.trace {
            // è®¡ç®—è¾“å‡ºå±‚çš„æ¢¯åº¦ dL/dOut
            // dL/dOut = 2 * (Pred - Target)
            let diff = hyper_tensor.root.translation.sub(&target_root.translation);
            let grad_output = AffineTuple::new(
                Matrix::new(MANIFOLD_DIM, MANIFOLD_DIM, vec![0.0; MANIFOLD_DIM*MANIFOLD_DIM]), // ç®€åŒ–: å¿½ç•¥çŸ©é˜µæ¢¯åº¦
                diff.scale(2.0)
            );

            // åå‘ä¼ æ’­åˆ°å¶å­èŠ‚ç‚¹
            let _leaf_grads = trace.backward(&grad_output);

            // 4. Update Weights (Optimizer Step)
            // åœ¨çœŸå®å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ® leaf_grads æ›´æ–°å¯¹åº”çš„ Embedding æˆ– Neuron æƒé‡
            // self.optimizer.step(&mut model_params, &leaf_grads);
        }

        loss
    }

    /// âš¡ Mode 2: Algebraic One-Shot Solver (ç¬é—´å­¦ä¹ )
    /// é€‚ç”¨äºè®°å¿†ç‰¹å®šäº‹å® (Memorization)
    /// "Input A + Input B -> Must imply Target C"
    pub fn train_step_solver(
        &mut self,
        neuron: &mut HTPNeuron, // ç›®æ ‡ç¥ç»å…ƒ
        input_state: &Vector,
        target_state: &Vector
    ) -> Float {
        // 1. Check current error
        let current_output = neuron.absorb(input_state);
        let initial_loss = LogicOracle::calculate_loss(&current_output, target_state);

        // å¦‚æœè¯¯å·®å·²ç»å¾ˆå°ï¼Œè·³è¿‡
        if initial_loss < self.params.tolerance_epsilon {
            return initial_loss;
        }

        // 2. Solve for Delta W (The Magic)
        // è¯¢é—® Oracleï¼šæˆ‘éœ€è¦æ€ä¹ˆæ”¹æƒé‡ï¼Œæ‰èƒ½è®© input å®Œç¾æ˜ å°„åˆ° targetï¼Ÿ
        let delta_w = LogicOracle::compute_ideal_update(
            input_state, 
            target_state, 
            &neuron.logic_gate
        );

        // 3. Apply Update Immediately
        // W_new = W_old + Delta_W * Learning_Rate
        // (Solver æ¨¡å¼ä¸‹ LR é€šå¸¸ä¸º 1.0ï¼Œå³å®Œå…¨æ¥å—å»ºè®®)
        let w_update = delta_w.scale(1.0); 
        neuron.logic_gate.linear = neuron.logic_gate.linear.add(&w_update);
        
        // åŒæ—¶ä¿®æ­£ Bias (Fix fixed-point drift)
        neuron.force_learn_bias(input_state, target_state);

        // 4. Verify
        let new_output = neuron.absorb(input_state);
        let final_loss = LogicOracle::calculate_loss(&new_output, target_state);

        final_loss
    }
}

/// ğŸ”§ SimpleOptimizer: åŸºç¡€æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
pub struct SimpleOptimizer {
    learning_rate: Float,
}

impl SimpleOptimizer {
    pub fn new(lr: Float) -> Self {
        SimpleOptimizer { learning_rate: lr }
    }

    /// W = W - lr * Grad
    pub fn apply_gradient(&self, weights: &mut Matrix, grad: &Matrix) {
        let step = grad.scale(-self.learning_rate);
        *weights = weights.add(&step);
    }
}
