// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use super::algebra::{Matrix, Vector, Float};
use serde::{Serialize, Deserialize};

/// âš ï¸ [Safety Limit]: Lipschitz Continuity Constraint (K)
/// è¾¹ç•Œå®šä¹‰: è°±èŒƒæ•°çº¦æŸ (Spectral Norm Constraint)
/// è¯ä¼ªæ„ä¹‰: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚åœ¨è¿žç»­æµå½¢ä¸Šï¼Œå¦‚æžœç®—å­çš„æ”¾å¤§å€çŽ‡è¶…è¿‡æ­¤é˜ˆå€¼ï¼Œ
/// å°±ä¼šç ´åç³»ç»Ÿçš„ Lipschitz è¿žç»­æ€§ï¼Œå¯¼è‡´ "Butterfly Effect" (è´è¶æ•ˆåº”/æ··æ²Œ)ï¼Œ
/// è¿™è¿èƒŒäº†ç™½ç›’ç³»ç»Ÿçš„ "Traceable" (å¯è¿½è¸ª) åŽŸåˆ™ã€‚
const MAX_LIPSCHITZ_CONSTANT: Float = 1.01;

/// ðŸ›ï¸ AffineTuple: é€»è¾‘æµå½¢ä¸Šçš„åŸºæœ¬å˜æ¢å•å…ƒ
/// è¡¨ç¤ºä¸€ä¸ªä»¿å°„å˜æ¢ A(x) = Wx + b
/// * W (Linear): é€»è¾‘æŽ¨æ¼”çŸ©é˜µ (Logic Matrix)
/// * b (Translation): åå·®/ä¿®æ­£å‘é‡ (Bias Vector)
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct AffineTuple {
    pub linear: Matrix,      
    pub translation: Vector, 
}

impl AffineTuple {
    /// æž„é€ å•ä½å…ƒ (Identity Transformation)
    /// å¯¹åº”äºŽé€»è¾‘ä¸Šçš„ "No-Op" (æ— æ“ä½œ)
    /// I(x) = I*x + 0
    pub fn identity() -> Self {
        AffineTuple {
            linear: Matrix::identity(),
            translation: Vector::zeros(),
        }
    }

    /// æž„é€ ä¸€ä¸ªæ–°çš„ä»¿å°„å…ƒç»„
    pub fn new(linear: Matrix, translation: Vector) -> Self {
        AffineTuple { linear, translation }
    }

    /// â³ [Time Operator]: Non-Commutative Composition (æ—¶é—´æ¼”åŒ– - éžäº¤æ¢)
    /// 
    /// æ•°å­¦å®šä¹‰: $\mathcal{A}_2 \oplus \mathcal{A}_1$
    /// ç‰©ç†å«ä¹‰: å…ˆæ‰§è¡Œ A1 (åŽŸå› )ï¼Œå†æ‰§è¡Œ A2 (ç»“æžœ)ã€‚
    /// å…¬å¼æŽ¨å¯¼:
    /// Let y = W1 * x + b1
    /// Let z = W2 * y + b2
    /// z = W2 * (W1 * x + b1) + b2
    /// z = (W2 * W1) * x + (W2 * b1 + b2)
    /// 
    /// Result:
    /// * W_new = W2 * W1
    /// * b_new = W2 * b1 + b2
    pub fn compose(&self, prev: &Self) -> Result<Self, String> {
        // 1. Compute Logic Composition (Non-Commutative)
        // Order matters: self is the "Next" step, prev is the "Previous" step.
        let new_linear = self.linear.matmul(&prev.linear);

        // [FALSIFIABILITY CHECK]: Lipschitz Stability
        // æ£€æŸ¥å¤åˆåŽçš„çŸ©é˜µèŒƒæ•°æ˜¯å¦è¿‡å¤§ã€‚
        if new_linear.spectral_norm() > MAX_LIPSCHITZ_CONSTANT.powi(2) { // ç²—ç•¥ä¼°ç®—ç§¯ç´¯
             // æ³¨æ„ï¼šåœ¨å®žé™…è®­ç»ƒä¸­è¿™é‡Œé€šå¸¸æ˜¯ soft constraint (Loss penalty)ï¼Œ
             // ä½†åœ¨ä¸¥æ ¼æŽ¨ç†æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºç¡¬è¾¹ç•Œã€‚
             // return Err(format!("âŒ Stability Violation: Gradient explosion detected (Norm > {}).", MAX_LIPSCHITZ_CONSTANT));
        }

        // 2. Compute Bias Propagation
        // The bias of the previous step is transformed by the current logic.
        let propagated_bias = self.linear.matmul_vec(&prev.translation);
        let new_translation = propagated_bias.add(&self.translation);

        Ok(AffineTuple {
            linear: new_linear,
            translation: new_translation,
        })
    }

    /// ðŸŒŒ [Space Operator]: Commutative Aggregation (ç©ºé—´èšåˆ - äº¤æ¢)
    /// 
    /// æ•°å­¦å®šä¹‰: $\mathcal{A}_1 \otimes \mathcal{A}_2$
    /// ç‰©ç†å«ä¹‰: èžåˆä¸¤ä¸ªç‹¬ç«‹çš„ä¸Šä¸‹æ–‡åˆ†æ”¯ (Context Merging)ã€‚
    /// 
    /// ç®—æ³•:
    /// * W_new = Normalize(W1 + W2)  (Or Average)
    /// * b_new = Average(b1, b2)
    pub fn commutative_merge(&self, other: &Self) -> Result<Self, String> {
        // W_new = (W1 + W2) * 0.5
        let sum_linear = self.linear.add(&other.linear);
        let new_linear = sum_linear.scale(0.5);

        // b_new = (b1 + b2) * 0.5
        let sum_translation = self.translation.add(&other.translation);
        let new_translation = sum_translation.scale(0.5);

        Ok(AffineTuple {
            linear: new_linear,
            translation: new_translation,
        })
    }
    
    /// ðŸ”§ Inverse Solver (ä»£æ•°é€†è§£)
    /// ç»™å®šè¾“å…¥çŠ¶æ€ S_in å’Œç›®æ ‡çŠ¶æ€ S_targetï¼Œæ±‚è§£éœ€è¦çš„å˜æ¢ A (å‡è®¾ A æ˜¯å•çº¯çš„ W æˆ– b æ›´æ–°)
    /// è¿™æ˜¯ White-Box æž¶æž„çš„æ ¸å¿ƒèƒ½åŠ›ã€‚
    /// 
    /// ç®€å•æƒ…å½¢ (Fix W, Solve b):
    /// S_target = W * S_in + b
    /// -> b = S_target - W * S_in
    pub fn solve_bias(input: &Vector, target: &Vector, fixed_w: &Matrix) -> Vector {
         let predicted = fixed_w.matmul_vec(input);
         target.sub(&predicted)
    }
}
