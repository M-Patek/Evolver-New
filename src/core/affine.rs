// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use super::algebra::{Matrix, Vector, Float};
use serde::{Serialize, Deserialize};

/// âš ï¸ [Safety Limit]: Lipschitz Continuity Constraint (K)
/// è¾¹ç•Œå®šä¹‰: è°±èŒƒæ•°çº¦æŸ (Spectral Norm Constraint)
/// è¯ä¼ªæ„ä¹‰: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚åœ¨è¿žç»­æµå½¢ä¸Šï¼Œå¦‚æžœç®—å­çš„æ”¾å¤§å€çŽ‡è¶…è¿‡æ­¤é˜ˆå€¼ï¼Œ
/// å°±ä¼šç ´åç³»ç»Ÿçš„ Lipschitz è¿žç»­æ€§ï¼Œå¯¼è‡´ "Butterfly Effect" (è´è¶æ•ˆåº”/æ··æ²Œ)ï¼Œ
/// è¿™è¿èƒŒäº†ç™½ç›’ç³»ç»Ÿçš„ "Traceable" (å¯è¿½è¸ª) åŽŸåˆ™ã€‚
const MAX_LIPSCHITZ_CONSTANT: Float = 1.01;

/// ðŸ›ï¸ AffineTuple: é€»è¾‘æµå½¢ä¸Šçš„åŸºæœ¬å˜æ¢å•å…ƒ
/// è¡¨ç¤ºä¸€ä¸ªä»¿å°„å˜æ¢ A(x) = Wx + b
/// * W (Linear): é€»è¾‘æŽ¨æ¼”çŸ©é˜µ (Logic Matrix)
/// * b (Translation): åå·®/ä¿®æ­£å‘é‡ (Bias Vector)
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct AffineTuple {
    pub linear: Matrix,      
    pub translation: Vector, 
}

impl AffineTuple {
    /// æž„é€ å•ä½å…ƒ (Identity Transformation)
    /// å¯¹åº”äºŽé€»è¾‘ä¸Šçš„ "No-Op" (æ— æ“ä½œ)
    /// I(x) = I*x + 0
    pub fn identity() -> Self {
        AffineTuple {
            linear: Matrix::identity(),
            translation: Vector::zeros(),
        }
    }
    
    /// æž„é€ é›¶å…ƒ (Zero Transformation)
    /// ç”¨äºŽç´¯åŠ å™¨çš„åˆå§‹çŠ¶æ€
    pub fn zeros() -> Self {
        // åˆ›å»ºå…¨0çŸ©é˜µå’Œå…¨0å‘é‡
        let zero_vec = Vector::zeros();
        let zero_mat = Matrix {
            rows: zero_vec.data.len(),
            cols: zero_vec.data.len(),
            data: vec![0.0; zero_vec.data.len() * zero_vec.data.len()]
        };
        AffineTuple {
            linear: zero_mat,
            translation: zero_vec,
        }
    }

    /// æž„é€ ä¸€ä¸ªæ–°çš„ä»¿å°„å…ƒç»„
    pub fn new(linear: Matrix, translation: Vector) -> Self {
        AffineTuple { linear, translation }
    }

    /// â³ [Time Operator]: Non-Commutative Composition (æ—¶é—´æ¼”åŒ– - éžäº¤æ¢)
    /// 
    /// æ•°å­¦å®šä¹‰: $\mathcal{A}_2 \oplus \mathcal{A}_1$
    /// ç‰©ç†å«ä¹‰: å…ˆæ‰§è¡Œ A1 (åŽŸå› )ï¼Œå†æ‰§è¡Œ A2 (ç»“æžœ)ã€‚
    /// å…¬å¼æŽ¨å¯¼:
    /// Let y = W1 * x + b1
    /// Let z = W2 * y + b2
    /// z = W2 * (W1 * x + b1) + b2
    /// z = (W2 * W1) * x + (W2 * b1 + b2)
    /// 
    /// Result:
    /// * W_new = W2 * W1
    /// * b_new = W2 * b1 + b2
    pub fn compose(&self, prev: &Self) -> Result<Self, String> {
        // 1. Compute Logic Composition (Non-Commutative)
        // Order matters: self is the "Next" step, prev is the "Previous" step.
        let new_linear = self.linear.matmul(&prev.linear);

        // [FALSIFIABILITY CHECK]: Lipschitz Stability
        // æ£€æŸ¥å¤åˆåŽçš„çŸ©é˜µèŒƒæ•°æ˜¯å¦è¿‡å¤§ã€‚
        if new_linear.spectral_norm() > MAX_LIPSCHITZ_CONSTANT.powi(2) { // ç²—ç•¥ä¼°ç®—ç§¯ç´¯
             // æ³¨æ„ï¼šåœ¨å®žé™…è®­ç»ƒä¸­è¿™é‡Œé€šå¸¸æ˜¯ soft constraint (Loss penalty)ï¼Œ
             // ä½†åœ¨ä¸¥æ ¼æŽ¨ç†æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºç¡¬è¾¹ç•Œã€‚
             // return Err(format!("âŒ Stability Violation: Gradient explosion detected (Norm > {}).", MAX_LIPSCHITZ_CONSTANT));
        }

        // 2. Compute Bias Propagation
        // The bias of the previous step is transformed by the current logic.
        let propagated_bias = self.linear.matmul_vec(&prev.translation);
        let new_translation = propagated_bias.add(&self.translation);

        Ok(AffineTuple {
            linear: new_linear,
            translation: new_translation,
        })
    }

    /// âž• [Primitive]: Pure Addition (çº¯åŠ æ³•)
    /// ç”¨äºŽæž„å»º Monoid ç»“æž„ã€‚ä¸åŒ…å«å¹³å‡é€»è¾‘ã€‚
    /// Math: (W1+W2, b1+b2)
    pub fn add_components(&self, other: &Self) -> Self {
        let new_linear = self.linear.add(&other.linear);
        let new_translation = self.translation.add(&other.translation);
        
        AffineTuple {
            linear: new_linear,
            translation: new_translation,
        }
    }
    
    /// ðŸ“ [Primitive]: Scalar Scaling (æ ‡é‡ç¼©æ”¾)
    /// ç”¨äºŽå½’ä¸€åŒ–æ­¥éª¤ã€‚
    pub fn scale(&self, factor: Float) -> Self {
        AffineTuple {
            linear: self.linear.scale(factor),
            translation: self.translation.scale(factor),
        }
    }

    /// ðŸŒŒ [Space Operator]: Commutative Aggregation (ç©ºé—´èšåˆ - äº¤æ¢)
    /// 
    /// æ•°å­¦å®šä¹‰: $\mathcal{A}_1 \otimes \mathcal{A}_2$
    /// ç‰©ç†å«ä¹‰: èžåˆä¸¤ä¸ªç‹¬ç«‹çš„ä¸Šä¸‹æ–‡åˆ†æ”¯ (Context Merging)ã€‚
    /// 
    /// âš ï¸ ä¿®æ­£æ³¨è®°: åŽŸå…ˆçš„å®žçŽ°ç›´æŽ¥å–å¹³å‡ (A+B)/2ï¼Œè¿™ç ´åäº†ç»“åˆå¾‹ã€‚
    /// çŽ°åœ¨å»ºè®®åœ¨ folding å±‚ä½¿ç”¨ Accumulatorï¼Œè¿™é‡Œä»…ä½œä¸ºä¼ ç»Ÿçš„äºŒå…ƒè¾…åŠ©å‡½æ•°ä¿ç•™ï¼Œ
    /// ä½†åº•å±‚é€»è¾‘å·²æ”¹ä¸ºä¾èµ– add_componentsã€‚
    pub fn commutative_merge(&self, other: &Self) -> Result<Self, String> {
        // ä½¿ç”¨çº¯åŠ æ³•åŽç¼©æ”¾ï¼Œé€»è¾‘ä¸Šç­‰ä»·äºŽ (A+B)/2
        let sum = self.add_components(other);
        Ok(sum.scale(0.5))
    }
    
    /// ðŸ”§ Inverse Solver (ä»£æ•°é€†è§£)
    /// ç»™å®šè¾“å…¥çŠ¶æ€ S_in å’Œç›®æ ‡çŠ¶æ€ S_targetï¼Œæ±‚è§£éœ€è¦çš„å˜æ¢ A (å‡è®¾ A æ˜¯å•çº¯çš„ W æˆ– b æ›´æ–°)
    /// è¿™æ˜¯ White-Box æž¶æž„çš„æ ¸å¿ƒèƒ½åŠ›ã€‚
    /// 
    /// ç®€å•æƒ…å½¢ (Fix W, Solve b):
    /// S_target = W * S_in + b
    /// -> b = S_target - W * S_in
    pub fn solve_bias(input: &Vector, target: &Vector, fixed_w: &Matrix) -> Vector {
         let predicted = fixed_w.matmul_vec(input);
         target.sub(&predicted)
    }
}
