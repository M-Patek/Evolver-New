// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use serde::{Serialize, Deserialize};

// ==================================================================
// 1. åŸºç¡€ç±»å‹å®šä¹‰ (The Manifold Substrate)
// ==================================================================

/// ğŸ¯ Precision Selection
pub type Float = f32;

/// ğŸ“ Manifold Dimension (D)
/// é€»è¾‘æµå½¢çš„ç»´åº¦ã€‚
pub const MANIFOLD_DIM: usize = 512;

/// ğŸ›ï¸ Vector: é€»è¾‘æµå½¢ä¸Šçš„ç‚¹æˆ–ä½ç§»å‘é‡
/// Represents a point $v \in \mathbb{R}^D$
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct Vector {
    pub data: Vec<Float>,
}

/// ğŸ›ï¸ Matrix: çº¿æ€§å˜æ¢ç®—å­
/// Represents a linear map $W: \mathbb{R}^D \to \mathbb{R}^D$
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct Matrix {
    pub rows: usize,
    pub cols: usize,
    pub data: Vec<Float>,
}

// ==================================================================
// 2. çº¿æ€§ä»£æ•°æ ¸å¿ƒå®ç° (Linear Algebra Kernel)
// ==================================================================

impl Vector {
    /// åˆ›å»ºæ–°å‘é‡ (éœ€è¦æ£€æŸ¥ç»´åº¦)
    pub fn new(data: Vec<Float>) -> Self {
        if data.len() != MANIFOLD_DIM {
            // åœ¨ä¸¥æ ¼æ¨¡å¼ä¸‹åº”è¯¥ panic æˆ–è¿”å› Result
            eprintln!("âš ï¸ Warning: Vector dimension mismatch. Expected {}, got {}", MANIFOLD_DIM, data.len());
        }
        Vector { data }
    }

    /// é›¶å‘é‡ (Origin)
    pub fn zeros() -> Self {
        Vector { data: vec![0.0; MANIFOLD_DIM] }
    }

    /// å‘é‡ L2 èŒƒæ•°
    pub fn norm(&self) -> Float {
        self.data.iter().map(|x| x * x).sum::<Float>().sqrt()
    }

    /// å½’ä¸€åŒ–å‘é‡
    pub fn normalize(&self) -> Self {
        let n = self.norm();
        if n < 1e-9 {
            return self.clone();
        }
        self.scale(1.0 / n)
    }

    /// å‘é‡åŠ æ³•: $v + u$
    pub fn add(&self, other: &Self) -> Self {
        let new_data = self.data.iter()
            .zip(&other.data)
            .map(|(a, b)| a + b)
            .collect();
        Vector { data: new_data }
    }

    /// å‘é‡å‡æ³•: $v - u$
    pub fn sub(&self, other: &Self) -> Self {
        let new_data = self.data.iter()
            .zip(&other.data)
            .map(|(a, b)| a - b)
            .collect();
        Vector { data: new_data }
    }

    /// æ ‡é‡ä¹˜æ³•: $k \cdot v$
    pub fn scale(&self, scalar: Float) -> Self {
        let new_data = self.data.iter()
            .map(|a| a * scalar)
            .collect();
        Vector { data: new_data }
    }

    /// åŸå§‹æ•°æ®è®¿é—®
    pub fn as_slice(&self) -> &[Float] {
        &self.data
    }
}

impl Matrix {
    /// åˆ›å»ºæ–°çŸ©é˜µ
    pub fn new(rows: usize, cols: usize, data: Vec<Float>) -> Self {
        assert_eq!(data.len(), rows * cols, "Matrix data size does not match dimensions");
        Matrix { rows, cols, data }
    }

    /// å•ä½çŸ©é˜µ (Identity Matrix)
    /// $I \cdot v = v$
    pub fn identity() -> Self {
        let mut data = vec![0.0; MANIFOLD_DIM * MANIFOLD_DIM];
        for i in 0..MANIFOLD_DIM {
            data[i * MANIFOLD_DIM + i] = 1.0;
        }
        Matrix { 
            rows: MANIFOLD_DIM, 
            cols: MANIFOLD_DIM, 
            data 
        }
    }

    /// çŸ©é˜µä¹˜æ³• (Matrix Multiplication): $C = A \cdot B$
    pub fn matmul(&self, other: &Self) -> Self {
        assert_eq!(self.cols, other.rows, "Matrix dimension mismatch for multiplication");
        let n = self.rows;
        let m = self.cols;
        let p = other.cols;
        
        let mut result = vec![0.0; n * p];
        
        // Naive implementation O(N^3)
        for i in 0..n {
            for k in 0..m {
                let r = self.data[i * m + k];
                if r.abs() > 1e-9 {
                    for j in 0..p {
                        result[i * p + j] += r * other.data[k * p + j];
                    }
                }
            }
        }
        
        Matrix { rows: n, cols: p, data: result }
    }

    /// çŸ©é˜µ-å‘é‡ä¹˜æ³• (Matrix-Vector Product): $y = A \cdot x$
    pub fn matmul_vec(&self, vec: &Vector) -> Vector {
        assert_eq!(self.cols, vec.data.len(), "Matrix-Vector dimension mismatch");
        let mut result = vec![0.0; self.rows];
        
        for i in 0..self.rows {
            let mut sum = 0.0;
            for j in 0..self.cols {
                sum += self.data[i * self.cols + j] * vec.data[j];
            }
            result[i] = sum;
        }
        
        Vector { data: result }
    }

    /// è½¬ç½®çŸ©é˜µ-å‘é‡ä¹˜æ³•: $y = A^T \cdot x$
    /// ç”¨äº Power Iteration
    pub fn transpose_matmul_vec(&self, vec: &Vector) -> Vector {
        assert_eq!(self.rows, vec.data.len(), "Matrix-Vector dimension mismatch for transpose");
        let mut result = vec![0.0; self.cols];

        for i in 0..self.rows {
            let val = vec.data[i];
            if val.abs() > 1e-9 {
                for j in 0..self.cols {
                    result[j] += self.data[i * self.cols + j] * val;
                }
            }
        }
        Vector { data: result }
    }

    /// çŸ©é˜µåŠ æ³• (Matrix Addition): $A + B$
    pub fn add(&self, other: &Self) -> Self {
        assert_eq!(self.data.len(), other.data.len(), "Matrix addition shape mismatch");
        let new_data = self.data.iter()
            .zip(&other.data)
            .map(|(a, b)| a + b)
            .collect();
        Matrix { rows: self.rows, cols: self.cols, data: new_data }
    }

    /// çŸ©é˜µç¼©æ”¾ (Scalar Multiplication): $k \cdot A$
    pub fn scale(&self, scalar: Float) -> Self {
        let new_data = self.data.iter()
            .map(|a| a * scalar)
            .collect();
        Matrix { rows: self.rows, cols: self.cols, data: new_data }
    }

    /// ğŸ“Š Frobenius Norm (åŸ spectral_norm)
    /// $\|A\|_F = \sqrt{\sum a_{ij}^2}$
    /// è¿™ä¸æ˜¯ Lipschitz å¸¸æ•°ï¼Œåªæ˜¯çŸ©é˜µå…ƒç´ çš„èƒ½é‡æ€»å’Œã€‚
    /// å¯¹äºå•ä½çŸ©é˜µï¼Œæ­¤å€¼ä¸º sqrt(D)ã€‚
    pub fn frobenius_norm(&self) -> Float {
        self.data.iter()
            .map(|x| x * x)
            .sum::<Float>()
            .sqrt()
    }

    /// ğŸ›¡ï¸ Estimated Spectral Norm (Power Iteration)
    /// ä¼°ç®—çŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼ $\sigma_{max}$ï¼Œå³çœŸå®çš„ Lipschitz å¸¸æ•°ã€‚
    /// ç®—æ³•ï¼šå¹‚è¿­ä»£æ³• (Power Method) ä½œç”¨äº $A^T A$ã€‚
    /// Iterations: é€šå¸¸ 3 æ¬¡å³å¯å¾—åˆ°å¯¹äºç¨³å®šæ€§æ£€æŸ¥è¶³å¤Ÿç²¾ç¡®çš„ä¸‹ç•Œä¼°è®¡ã€‚
    pub fn estimate_spectral_norm(&self, iterations: usize) -> Float {
        // 1. åˆå§‹åŒ–æ¢æµ‹å‘é‡ (Deterministically)
        // ä½¿ç”¨å‡åŒ€åˆ†å¸ƒçš„å‘é‡è€Œä¸æ˜¯éšæœºå‘é‡ï¼Œç¡®ä¿ç¡®å®šæ€§ã€‚
        let init_val = 1.0 / (self.cols as Float).sqrt();
        let mut v = Vector::new(vec![init_val; self.cols]);

        // 2. Power Iteration: v_k = A^T * A * v_{k-1}
        for _ in 0..iterations {
            let av = self.matmul_vec(&v);         // Apply A
            let at_av = self.transpose_matmul_vec(&av); // Apply A^T
            v = at_av.normalize();                // Re-normalize
        }

        // 3. Compute Rayleigh Quotient Approximation
        // sigma ~ ||A v||
        let av = self.matmul_vec(&v);
        av.norm()
    }
}
