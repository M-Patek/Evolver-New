// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use std::sync::Arc;
use tokio::sync::RwLock;
use log::{info, warn, error};

use crate::core::algebra::{Vector, Matrix};
use crate::core::affine::AffineTuple;
use crate::core::neuron::HTPNeuron;
use crate::core::oracle::LogicOracle;
use crate::topology::tensor::HyperTensor;
use crate::net::wire::{PacketType, GradientUpdate, ModelSnapshot, LayerState};
use crate::train_loop::SimpleOptimizer;

/// ğŸ­ NodeRole: èŠ‚ç‚¹èº«ä»½
#[derive(Debug, Clone, PartialEq)]
pub enum NodeRole {
    /// ğŸ‘· Worker: è´Ÿè´£æ‰§è¡Œå‰å‘æ¨ç†å’Œåå‘ä¼ æ’­è®¡ç®—
    Worker,
    /// ğŸ§  ParameterServer: è´Ÿè´£ç»´æŠ¤å…¨å±€çœŸç†ï¼ˆæƒé‡ï¼‰å¹¶æ‰§è¡Œæ›´æ–°
    ParameterServer,
}

/// ğŸ¤– HTPNode: ç¥ç»èŠ‚ç‚¹å®ä½“
pub struct HTPNode {
    pub id: String,
    pub role: NodeRole,
    
    /// ğŸ§  Local Memory: æœ¬åœ°å­˜å‚¨çš„ç¥ç»ç½‘ç»œæ¨¡å‹
    /// Worker å­˜çš„æ˜¯å‰¯æœ¬ (Cache)ï¼ŒPS å­˜çš„æ˜¯çœŸç† (Master)
    /// ä½¿ç”¨ Arc<RwLock> å®ç°çº¿ç¨‹å®‰å…¨çš„å¹¶å‘è®¿é—®
    pub model: Arc<RwLock<Vec<HTPNeuron>>>,

    /// âš¡ Optimizer: ä»… PS èŠ‚ç‚¹æŒæœ‰ï¼Œç”¨äºæ›´æ–°æƒé‡
    pub optimizer: Option<SimpleOptimizer>,
}

impl HTPNode {
    /// åˆå§‹åŒ–ä¸€ä¸ªæ–°èŠ‚ç‚¹
    pub fn new(id: String, role: NodeRole, model_depth: usize) -> Self {
        // åˆå§‹åŒ–ç©ºç™½æ¨¡å‹ (å®é™…åº”ç”¨ä¸­åº”ä»ç£ç›˜åŠ è½½æˆ–é€šè¿‡ç½‘ç»œåŒæ­¥)
        let mut neurons = Vec::with_capacity(model_depth);
        for _ in 0..model_depth {
            neurons.push(HTPNeuron::new());
        }

        let optimizer = match role {
            NodeRole::ParameterServer => Some(SimpleOptimizer::new(1e-3)), // é»˜è®¤å­¦ä¹ ç‡
            NodeRole::Worker => None,
        };

        HTPNode {
            id,
            role,
            model: Arc::new(RwLock::new(neurons)),
            optimizer,
        }
    }

    /// ğŸ“¨ Packet Processor: æ ¸å¿ƒæ¶ˆæ¯å¤„ç†å¾ªç¯
    /// æ¨¡æ‹Ÿæ¥æ”¶åˆ°ä¸€ä¸ªç½‘ç»œåŒ…å¹¶å¤„ç† (å®é™…åº”é…åˆ Quinn/Tokio Stream ä½¿ç”¨)
    pub async fn process_packet(&self, packet: PacketType) -> Option<PacketType> {
        match packet {
            PacketType::Handshake { node_id, protocol_ver } => {
                info!("ğŸ¤ Handshake received from [{}] (v{})", node_id, protocol_ver);
                // è¿™é‡Œå¯ä»¥è¿”å›ä¸€ä¸ª HandshakeAckï¼Œæš‚æ—¶ç•¥è¿‡
                None
            }

            PacketType::InferenceRequest { request_id, input_state } => {
                if self.role != NodeRole::Worker {
                    warn!("âš ï¸ PS received InferenceRequest. Ignoring.");
                    return None;
                }
                self.handle_inference(request_id, input_state).await
            }

            PacketType::GradientPush(grad) => {
                if self.role != NodeRole::ParameterServer {
                    warn!("âš ï¸ Worker received GradientPush. Ignoring.");
                    return None;
                }
                self.handle_gradient_update(grad).await
            }

            PacketType::ParameterBroadcast(snapshot) => {
                if self.role != NodeRole::Worker {
                    return None; // PS é€šå¸¸ä¸æ¥æ”¶å¹¿æ’­ï¼Œé™¤éæ˜¯å¤šçº§ PS æ¶æ„
                }
                self.handle_parameter_sync(snapshot).await
            }

            _ => None,
        }
    }

    /// ğŸ§  [Worker Logic]: æ‰§è¡Œæ¨ç†
    async fn handle_inference(&self, request_id: u64, input: Vector) -> Option<PacketType> {
        info!("ğŸ§  Worker [{}] processing Request #{}", self.id, request_id);

        let model_guard = self.model.read().await;
        
        // 1. æ„å»ºè®¡ç®—å›¾è¾“å…¥
        // è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå‡è®¾æ¨¡å‹æ˜¯å•å±‚æˆ–ç®€å•çš„ä¸²è¡Œç»“æ„ï¼Œå°†è¾“å…¥åŒ…è£…ä¸º AffineTuple
        // å®é™…çš„ Evolver ä¼šæ„å»ºå¤æ‚çš„ HyperTensor
        let input_tuple = AffineTuple::new(Matrix::identity(), input);
        
        // 2. æ¨¡æ‹Ÿç½‘ç»œå‰å‘ä¼ æ’­ (Forward Pass)
        // è¿™é‡Œçš„é€»è¾‘æ˜¯å°†è¾“å…¥é€šè¿‡æ‰€æœ‰ç¥ç»å…ƒæŠ˜å ã€‚
        // ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ªç¥ç»å…ƒè¿›è¡Œå¤„ç†ã€‚
        let mut result_vector = Vector::zeros();
        if let Some(first_neuron) = model_guard.first() {
             // Clone ç¥ç»å…ƒçŠ¶æ€ä»¥é¿å…ç”±äºå€Ÿç”¨æ£€æŸ¥å™¨å¯¼è‡´çš„å†²çªï¼Œ
             // åœ¨å®é™…é«˜æ€§èƒ½åœºæ™¯ä¸‹åº”ä½¿ç”¨ Zero-copyã€‚
             let mut neuron_clone = first_neuron.clone(); 
             result_vector = neuron_clone.absorb(&input_tuple.translation);
        }

        // 3. è¿”å›ç»“æœ
        Some(PacketType::InferenceResponse {
            request_id,
            output_state: result_vector,
        })
    }

    /// ğŸ“‰ [PS Logic]: æ¢¯åº¦ä¸‹é™æ›´æ–°
    async fn handle_gradient_update(&self, grad: GradientUpdate) -> Option<PacketType> {
        info!("ğŸ“‰ PS [{}] applying gradients to Layer {}", self.id, grad.layer_index);

        if let Some(opt) = &self.optimizer {
            let mut model_guard = self.model.write().await;
            
            if let Some(target_neuron) = model_guard.get_mut(grad.layer_index) {
                // 1. é‡æ„æ¢¯åº¦çŸ©é˜µ
                // GradientUpdate ä¼ è¾“çš„æ˜¯æ‰å¹³åŒ–çš„ Vec<Float>ï¼Œéœ€è¦è¿˜åŸä¸º Matrix
                let weight_grad_mat = Matrix::new(
                    target_neuron.logic_gate.linear.rows,
                    target_neuron.logic_gate.linear.cols,
                    grad.weight_grad
                );

                // 2. æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤ (W = W - lr * grad)
                opt.apply_gradient(&mut target_neuron.logic_gate.linear, &weight_grad_mat);
                
                // 3. æ›´æ–° Bias (ç®€å•ç›¸å‡)
                // å®é™… SimpleOptimizer ä¹Ÿåº”è¯¥æ”¯æŒ Biasï¼Œè¿™é‡Œæ‰‹åŠ¨æ¼”ç¤º
                let bias_grad_vec = Vector::new(grad.bias_grad);
                let lr = 1e-3; // æš‚æ—¶ç¡¬ç¼–ç ï¼Œåº”ä» params è¯»å–
                target_neuron.logic_gate.translation = target_neuron.logic_gate.translation
                    .sub(&bias_grad_vec.scale(lr));

                info!("âœ… Weights updated via Gradient Descent.");
                
                // 4. (å¯é€‰) è§¦å‘å¹¿æ’­ï¼šå¦‚æœæ›´æ–°ç´¯è®¡åˆ°ä¸€å®šç¨‹åº¦ï¼Œå¹¿æ’­æ–°å‚æ•°
                // è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæ¯æ¬¡æ›´æ–°éƒ½å¹¿æ’­ï¼ˆæ•ˆç‡æä½ï¼Œä»…ä½œé€»è¾‘å±•ç¤ºï¼‰
                return Some(self.create_snapshot(&model_guard));
            }
        }
        None
    }

    /// ğŸ§¬ [Worker Logic]: åŒæ­¥å…¨å±€å‚æ•°
    async fn handle_parameter_sync(&self, snapshot: ModelSnapshot) -> Option<PacketType> {
        info!("ğŸ§¬ Worker [{}] syncing with Global Truth (Epoch {})", self.id, snapshot.epoch);
        
        let mut model_guard = self.model.write().await;
        
        for layer_state in snapshot.layers {
            if layer_state.layer_index < model_guard.len() {
                // è¦†ç›–æœ¬åœ°æƒé‡
                model_guard[layer_state.layer_index].logic_gate.linear = layer_state.weights;
                model_guard[layer_state.layer_index].logic_gate.bias = layer_state.bias; // ä¿®æ­£: LayerState å®šä¹‰é‡Œæ˜¯ bias
            }
        }
        None
    }

    /// ğŸ“¸ Helper: åˆ›å»ºæ¨¡å‹å¿«ç…§
    fn create_snapshot(&self, neurons: &[HTPNeuron]) -> PacketType {
        let layers = neurons.iter().enumerate().map(|(idx, n)| {
            LayerState {
                layer_index: idx,
                weights: n.logic_gate.linear.clone(),
                bias: n.logic_gate.translation.clone(),
            }
        }).collect();

        PacketType::ParameterBroadcast(ModelSnapshot {
            epoch: 0, // å®é™…åº”ç»´æŠ¤å…¨å±€ Epoch è®¡æ•°å™¨
            layers,
        })
    }
}
