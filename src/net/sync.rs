// COPYRIGHT (C) 2025 M-Patek. ALL RIGHTS RESERVED.

use std::collections::{HashMap, HashSet};
use crate::core::algebra::{Matrix, Vector, Float};
use crate::net::wire::GradientUpdate;

/// ğŸ“Š AggregationResult: èšåˆå™¨çš„è¾“å‡º
pub enum AggregationResult {
    /// â³ å°šæœªæ”¶é½ï¼Œç»§ç»­ç­‰å¾…
    Pending,
    /// âœ… å·²æ”¶é½ï¼Œè¾“å‡ºèšåˆåçš„æ¢¯åº¦ï¼ˆå‡†å¤‡å‘ç»™çˆ¶èŠ‚ç‚¹æˆ–åº”ç”¨åˆ°æ¨¡å‹ï¼‰
    Complete(GradientUpdate),
    /// âš ï¸ è¿™æ˜¯ä¸€ä¸ªè¿‡æœŸçš„æ¢¯åº¦ï¼ˆEpoch è½åï¼‰ï¼Œå·²ä¸¢å¼ƒ
    Stale,
}

/// ğŸ§  LayerAccumulator: å•å±‚çš„ç´¯åŠ å™¨
/// è´Ÿè´£å¤„ç† (g1*n1 + g2*n2) / (n1+n2) çš„åŠ æƒé€»è¾‘
struct LayerAccumulator {
    /// ç´¯ç§¯çš„æƒé‡æ¢¯åº¦å’Œ (Î£ g_w * n)
    weighted_sum_w: Vec<Float>,
    /// ç´¯ç§¯çš„åç½®æ¢¯åº¦å’Œ (Î£ g_b * n)
    weighted_sum_b: Vec<Float>,
    /// æ€»æ ·æœ¬æ•° (Î£ n)
    total_batch: usize,
    /// å·²è´¡çŒ®çš„èŠ‚ç‚¹ ID é›†åˆ (é˜²é‡å¤æäº¤)
    contributors: HashSet<String>,
}

impl LayerAccumulator {
    fn new() -> Self {
        LayerAccumulator {
            weighted_sum_w: Vec::new(),
            weighted_sum_b: Vec::new(),
            total_batch: 0,
            contributors: HashSet::new(),
        }
    }

    /// â• å¸æ”¶ä¸€ä¸ªæ–°çš„æ¢¯åº¦åŒ…
    fn absorb(&mut self, grad: &GradientUpdate, from_node: &str) {
        if self.contributors.contains(from_node) {
            return; // å¹‚ç­‰æ€§ä¿æŠ¤ï¼šå¿½ç•¥é‡å¤æäº¤
        }

        let n = grad.batch_size as Float;

        // 1. åˆå§‹åŒ–æˆ–ç´¯åŠ  Weight æ¢¯åº¦
        if self.weighted_sum_w.is_empty() {
            // Init: g * n
            self.weighted_sum_w = grad.weight_grad.iter().map(|&g| g * n).collect();
        } else {
            // Accumulate: += g * n
            for (i, &g) in grad.weight_grad.iter().enumerate() {
                if i < self.weighted_sum_w.len() {
                    self.weighted_sum_w[i] += g * n;
                }
            }
        }

        // 2. åˆå§‹åŒ–æˆ–ç´¯åŠ  Bias æ¢¯åº¦
        if self.weighted_sum_b.is_empty() {
            self.weighted_sum_b = grad.bias_grad.iter().map(|&g| g * n).collect();
        } else {
            for (i, &g) in grad.bias_grad.iter().enumerate() {
                if i < self.weighted_sum_b.len() {
                    self.weighted_sum_b[i] += g * n;
                }
            }
        }

        self.total_batch += grad.batch_size;
        self.contributors.insert(from_node.to_string());
    }

    /// â— å½’ä¸€åŒ–å¹¶è¾“å‡ºæœ€ç»ˆæ¢¯åº¦
    /// New_Avg = Sum(Weighted_Grads) / Total_Batch
    fn finalize(&self, layer_idx: usize) -> GradientUpdate {
        let scale = if self.total_batch > 0 {
            1.0 / (self.total_batch as Float)
        } else {
            1.0
        };

        GradientUpdate {
            layer_index: layer_idx,
            weight_grad: self.weighted_sum_w.iter().map(|&x| x * scale).collect(),
            bias_grad: self.weighted_sum_b.iter().map(|&x| x * scale).collect(),
            batch_size: self.total_batch,
        }
    }
}

/// ğŸŒŠ GradientAggregator: æ¢¯åº¦åŒæ­¥èšåˆå™¨
/// ç®¡ç†æ‰€æœ‰å±‚çº§çš„èšåˆçŠ¶æ€
pub struct GradientAggregator {
    /// å…¨å±€ Epoch è®¡æ•°å™¨ (é˜²æ­¢æ¥æ”¶ä¸Šä¸€è½®çš„å»¶è¿ŸåŒ…)
    current_epoch: u64,
    
    /// ç¼“å†²åŒº: LayerIndex -> Accumulator
    buffers: HashMap<usize, LayerAccumulator>,
}

impl GradientAggregator {
    pub fn new() -> Self {
        GradientAggregator {
            current_epoch: 0,
            buffers: HashMap::new(),
        }
    }

    /// ğŸ”„ è®¾ç½®æ–°çºªå…ƒ (æ¸…ç©ºæ—§ç¼“å†²)
    pub fn advance_epoch(&mut self, new_epoch: u64) {
        if new_epoch > self.current_epoch {
            self.current_epoch = new_epoch;
            self.buffers.clear();
        }
    }

    /// ğŸ“¥ å¤„ç†æ¢¯åº¦æ›´æ–°
    ///
    /// * `grad`: æ”¶åˆ°çš„æ¢¯åº¦åŒ…
    /// * `from_node`: æ¥æºèŠ‚ç‚¹ ID (å¦‚æœæ˜¯è‡ªå·±äº§ç”Ÿçš„ï¼Œå¯ä¼  "SELF")
    /// * `expected_children`: æ ¹æ®æ‹“æ‰‘ï¼Œæˆ‘åº”è¯¥ç­‰å¾…å“ªäº›å­èŠ‚ç‚¹ (ID List)
    pub fn aggregate(
        &mut self, 
        grad: GradientUpdate, 
        from_node: String, 
        expected_children: &[String]
    ) -> AggregationResult {
        // ç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾ GradientUpdate ç»“æ„é‡Œæœªæ¥åº”è¯¥å¸¦ epoch å­—æ®µã€‚
        // ç›®å‰å‡è®¾ç½‘ç»œæ˜¯åŒæ­¥çš„ï¼Œåªå¤„ç†å½“å‰é€»è¾‘ã€‚
        
        let layer_idx = grad.layer_index;
        
        // 1. è·å–æˆ–åˆ›å»ºç´¯åŠ å™¨
        let acc = self.buffers
            .entry(layer_idx)
            .or_insert_with(LayerAccumulator::new);

        // 2. å¸æ”¶æ¢¯åº¦
        acc.absorb(&grad, &from_node);

        // 3. æ£€æŸ¥å®Œæ•´æ€§ (Completeness Check)
        // æˆ‘ä»¬éœ€è¦ç­‰å¾…ï¼šæ‰€æœ‰å­èŠ‚ç‚¹ + æˆ‘è‡ªå·± ("SELF")
        // expected_count = children.len() + 1
        let mut all_needed: HashSet<String> = expected_children.iter().cloned().collect();
        all_needed.insert("SELF".to_string()); // å¿…é¡»åŒ…å«æœ¬åœ°è®¡ç®—çš„æ¢¯åº¦

        if acc.contributors.is_superset(&all_needed) {
            // âœ… å¬å”¤ç¥é¾™ï¼šæ‰€æœ‰ç¢ç‰‡å·²é›†é½
            let final_grad = acc.finalize(layer_idx);
            
            // æ¸…ç†ç¼“å†²åŒº (è¯¥å±‚æœ¬è½®å·²å®Œæˆ)
            self.buffers.remove(&layer_idx);
            
            return AggregationResult::Complete(final_grad);
        }

        AggregationResult::Pending
    }
}
